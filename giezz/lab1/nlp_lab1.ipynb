{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "150326cf",
   "metadata": {},
   "source": [
    "## Лабораторная работа 1. Морфологический парсер mystem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f96232",
   "metadata": {},
   "source": [
    "**Задание 1.** Изучите документацию и лицензию (!) морфологического парсера mystem от Yandex: https://yandex.ru/dev/mystem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fccc662",
   "metadata": {},
   "source": [
    "**Задание 2.** Установите `pymystem3` – интерфейс к mystem на Python: https://pypi.org/project/pymystem3.\n",
    "\n",
    "(!) Обратите внимание, что у конструктора объекта Mystem() есть параметры."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc49582",
   "metadata": {},
   "source": [
    "**Задание 3.** Выпишите с какими параметрами запускается морфологический анализатор.\n",
    "\n",
    "*Ваш ответ:*\n",
    "\n",
    "**а)** Придумайте и запишите примеры предложений со словами не из словаря. Приведите их полученные морфологические разборы.\n",
    "\n",
    "*Ваш ответ:*\n",
    "\n",
    "**б)** Применяется ли контекстное снятие омонимии при морфологическом разборе?\n",
    "\n",
    "*Ваш ответ:*"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T12:21:26.531373Z",
     "start_time": "2025-09-13T12:21:26.043759Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pymystem3 import Mystem\n",
    "text = \"Я решил запушить лонг, но меня закидали флешками и я был в пермоблайнде\"\n",
    "m = Mystem(disambiguation=True)\n",
    "lemmas = m.lemmatize(text)\n",
    "print(''.join(lemmas))"
   ],
   "id": "8bec6d8efd0dc287",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "я решать запушить лонг, но я закидывать флешка и я быть в пермоблайнде\n",
      "\n"
     ]
    }
   ],
   "execution_count": 73
  },
  {
   "cell_type": "markdown",
   "id": "fa18ab99",
   "metadata": {},
   "source": [
    "**Задание 4.** Напишите функцию `parse_text()`, на вход которой поступает текст (в виде строки), а на выходе формируется структура данных, содержащая для каждого слова входного текста следующую информацию:\n",
    "- исходную словоформу (wordform);\n",
    "- нормальную форму слова (лемму) (norm, lemma);\n",
    "- часть речи (part of speech, POS);\n",
    "- другую грамматическую информацию, выдаваемую mystem;\n",
    "- признак, присутствует ли слово в словаре mystem.\n",
    "\n",
    "Функция должна выбирать наиболее вероятный вариант морфологического разбора слова."
   ]
  },
  {
   "cell_type": "code",
   "id": "0fa032ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T12:21:26.537861Z",
     "start_time": "2025-09-13T12:21:26.534884Z"
    }
   },
   "source": [
    "from pymystem3 import Mystem\n",
    "\n",
    "def parse_text(text):\n",
    "    m = Mystem(disambiguation=True)\n",
    "    analysis = m.analyze(text)\n",
    "    result = []\n",
    "\n",
    "    for item in analysis:\n",
    "        if 'analysis' in item and item['analysis']:\n",
    "            best_parse = item['analysis'][0]\n",
    "            result.append({\n",
    "                'wordform': item['text'],\n",
    "                'lemma': best_parse.get('lex', item['text']),\n",
    "                'pos': best_parse['gr'].split(',')[0] if 'gr' in best_parse else None,\n",
    "                'gram_info': best_parse.get('gr', ''),\n",
    "                'in_dict': True\n",
    "            })\n",
    "        else:\n",
    "            result.append({\n",
    "                'wordform': item['text'],\n",
    "                'lemma': item['text'],\n",
    "                'pos': None,\n",
    "                'gram_info': '',\n",
    "                'in_dict': False\n",
    "            })\n",
    "    return result"
   ],
   "outputs": [],
   "execution_count": 74
  },
  {
   "cell_type": "markdown",
   "id": "45237dc3",
   "metadata": {},
   "source": [
    "**Задание 5.** Напишите функцию `save_morph_results()`, сохраняющую структуру данных, получаемую функцией `parse_text()`, в текстовый файл формата JSON."
   ]
  },
  {
   "cell_type": "code",
   "id": "912ddbe9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T12:21:26.547509Z",
     "start_time": "2025-09-13T12:21:26.542541Z"
    }
   },
   "source": [
    "import json\n",
    "\n",
    "def save_morph_results(data, filename):\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)"
   ],
   "outputs": [],
   "execution_count": 75
  },
  {
   "cell_type": "markdown",
   "id": "621548a6",
   "metadata": {},
   "source": [
    "**Задание 6.** Напишите функцию `get_dictionary()`, на вход которой поступает текст (в виде строки), а на выходе формируется словарь,\n",
    "включающий все уникальные слова текста и содержащий для каждого слова следующую информацию:\n",
    "- нормальную форму слова;\n",
    "- часть речи;\n",
    "- частоту слова в тексте;\n",
    "- все варианты словоформ в тексте с данной нормальной формой."
   ]
  },
  {
   "cell_type": "code",
   "id": "bf5fdae8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T12:21:26.557560Z",
     "start_time": "2025-09-13T12:21:26.550294Z"
    }
   },
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def get_dictionary(text):\n",
    "    parsed = parse_text(text)\n",
    "    word_freq = Counter()\n",
    "    lemmas_info = {}\n",
    "\n",
    "    for item in parsed:\n",
    "        word = item['wordform'].lower()\n",
    "        lemma = item['lemma']\n",
    "        pos = item['pos']\n",
    "\n",
    "        if item['in_dict']:\n",
    "            word_freq[lemma] += 1\n",
    "            if lemma not in lemmas_info:\n",
    "                lemmas_info[lemma] = {'pos': pos, 'forms': set()}\n",
    "            lemmas_info[lemma]['forms'].add(word)\n",
    "\n",
    "    dictionary = {}\n",
    "    for lemma, info in lemmas_info.items():\n",
    "        dictionary[lemma] = {\n",
    "            'pos': info['pos'],\n",
    "            'frequency': word_freq[lemma],\n",
    "            'wordforms': list(info['forms'])\n",
    "        }\n",
    "\n",
    "    return dictionary"
   ],
   "outputs": [],
   "execution_count": 76
  },
  {
   "cell_type": "markdown",
   "id": "4eeb0797",
   "metadata": {},
   "source": [
    "**Задание 7.** Напишите функцию `save_dictionary()`, сохраняющую предыдущую структуру данных в текстовый файл формата JSON. Слова в файле должны быть упорядочены по убыванию частоты."
   ]
  },
  {
   "cell_type": "code",
   "id": "10a36600",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T12:21:26.567561Z",
     "start_time": "2025-09-13T12:21:26.560086Z"
    }
   },
   "source": [
    "def save_dictionary(dictionary, filename):\n",
    "    sorted_dict = dict(sorted(dictionary.items(), key=lambda x: x[1]['frequency'], reverse=True))\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(sorted_dict, f, ensure_ascii=False, indent=2)"
   ],
   "outputs": [],
   "execution_count": 77
  },
  {
   "cell_type": "markdown",
   "id": "b86bdd0c",
   "metadata": {},
   "source": [
    "**Задание 8.** Напишите функцию `get_non_mystem_dict()`, на вход которой поступает структура данных, получаемая функцией `parse_text()`, а на выходе формируется словарь, содержащий уникальные слова текста, отсутствующие в словаре mystem, вместе с частотой слова в тексте."
   ]
  },
  {
   "cell_type": "code",
   "id": "a7299673",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T12:21:26.576713Z",
     "start_time": "2025-09-13T12:21:26.570927Z"
    }
   },
   "source": [
    "def get_non_mystem_dict(parsed_data):\n",
    "    non_dict_words = Counter()\n",
    "    for item in parsed_data:\n",
    "        if not item['in_dict'] and item['wordform'].strip():\n",
    "            non_dict_words[item['wordform']] += 1\n",
    "    return non_dict_words"
   ],
   "outputs": [],
   "execution_count": 78
  },
  {
   "cell_type": "markdown",
   "id": "9d3d5fc1",
   "metadata": {},
   "source": [
    "**Задание 9.** Напишите функцию `save_non_mystem_dict()`, сохраняющую структуру данных, получаемую функцией `get_non_mystem_dict()`, в текстовый файл формата TSV (tab-separated values). Слова в файле должны быть упорядочены по убыванию частоты."
   ]
  },
  {
   "cell_type": "code",
   "id": "9e29ebb1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T12:21:26.586725Z",
     "start_time": "2025-09-13T12:21:26.579809Z"
    }
   },
   "source": [
    "def save_non_mystem_dict(non_dict_words, filename):\n",
    "    sorted_words = non_dict_words.most_common()\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        for word, freq in sorted_words:\n",
    "            f.write(f\"{word}\\t{freq}\\n\")"
   ],
   "outputs": [],
   "execution_count": 79
  },
  {
   "cell_type": "markdown",
   "id": "f6945245",
   "metadata": {},
   "source": [
    "**Задание 10.** Напишите функцию `get_pos_distribution()`, на вход которой поступает словарь, формируемый функцией `get_dictionary()`, а на выходе выдается структура данных, содержащая частотное распределение частей речи в словаре со следующими значениями\n",
    "\n",
    "\n",
    "|часть речи|количество уникальных слов|общее количество слов|\n",
    "| -------- | ------------------------ | ------------------- |"
   ]
  },
  {
   "cell_type": "code",
   "id": "693be778",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T12:21:26.596250Z",
     "start_time": "2025-09-13T12:21:26.590388Z"
    }
   },
   "source": [
    "def get_pos_distribution(dictionary):\n",
    "    pos_count = defaultdict(lambda: {'unique': 0, 'total': 0})\n",
    "    for lemma, info in dictionary.items():\n",
    "        pos = info['pos']\n",
    "        pos_count[pos]['unique'] += 1\n",
    "        pos_count[pos]['total'] += info['frequency']\n",
    "    return pos_count"
   ],
   "outputs": [],
   "execution_count": 80
  },
  {
   "cell_type": "markdown",
   "id": "aa8e4f01",
   "metadata": {},
   "source": [
    "**Задание 11.** Проведите эксперименты с разработанными функциями:\n",
    "- скачайте 10 файлов с текстами разных жанров и разного размера (например, произведения классиков, современных писателей, новостные статьи, научные статьи и т.п.). *Учитывайте кодировку* – все файлы должны быть в UTF-8;\n",
    "- обработайте файлы при помощи функций `parse_text()`, `get_dictionary()` и `get_non_mystem_dict()`, и сохраните результаты в текстовых файлах при помощи функций `save_morph_results()`, `save_dictionary()` и `save_non_mystem_dict()`. Измеряйте время запуска функций! (см. следующий пункт);\n",
    "- заполните следующую таблицу:\n",
    "\n",
    "|Файл|Размер, байт|Размер текста (кол-во слов)|Размер словаря (кол-во уникальных слов)|Время работы get_dictionary(), сек.|\n",
    "|----|------------|---------------------------|---------------------------------------|-----------------------------------|\n",
    "- для самого большого словаря постройте частотное распределение слов:\n",
    "  - по оси ординат – частота,\n",
    "  - по оси абсцисс – слова, упорядоченные по убыванию частоты (по-другому, ранги слов);\n",
    "- постройте график зависимость времени морфологического анализа от размера текстового файла;\n",
    "- распределение частей речи, полученное функцией `get_pos_distribution()`, выведите на экран в виде таблицы и графика.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "e0e8c689",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T12:28:49.493053Z",
     "start_time": "2025-09-13T12:21:26.600915Z"
    }
   },
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from pymystem3 import Mystem\n",
    "\n",
    "files = [\n",
    "    'text1.txt', 'text2.txt', 'text3.txt',\n",
    "    'text4.txt', 'text5.txt'\n",
    "]\n",
    "\n",
    "results = []\n",
    "all_dictionaries = []\n",
    "\n",
    "for file in files:\n",
    "    if not os.path.exists(file):\n",
    "        print(f\"Файл {file} не найден, пропускаем\")\n",
    "        continue\n",
    "\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "\n",
    "    start_time = time.time()\n",
    "    dictionary = get_dictionary(text)\n",
    "    elapsed_time = time.time() - start_time\n",
    "\n",
    "    results.append({\n",
    "        'file': file,\n",
    "        'size_bytes': os.path.getsize(file),\n",
    "        'word_count': len(text.split()),\n",
    "        'unique_words': len(dictionary),\n",
    "        'time': elapsed_time\n",
    "    })\n",
    "\n",
    "    all_dictionaries.append(dictionary)\n",
    "\n",
    "    save_dictionary(dictionary, f'dict_{file}.json')\n",
    "\n",
    "    parsed_data = parse_text(text)\n",
    "    save_morph_results(parsed_data, f'parsed_{file}.json')\n",
    "\n",
    "    non_dict_words = get_non_mystem_dict(parsed_data)\n",
    "    save_non_mystem_dict(non_dict_words, f'non_dict_{file}.tsv')\n",
    "\n",
    "print(\"Результаты анализа файлов:\")\n",
    "for res in results:\n",
    "    print(f\"{res['file']}: {res['size_bytes']} байт, {res['word_count']} слов, \"\n",
    "          f\"{res['unique_words']} уникальных слов, время: {res['time']:.2f} сек.\")\n",
    "\n",
    "largest_dict = max(all_dictionaries, key=len)\n",
    "\n",
    "frequencies = sorted([item['frequency'] for item in largest_dict.values()], reverse=True)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(frequencies)\n",
    "plt.title('Частотное распределение слов')\n",
    "plt.xlabel('Ранг слова')\n",
    "plt.ylabel('Частота')\n",
    "plt.savefig('word_frequency.png')\n",
    "plt.show()\n",
    "\n",
    "sizes = [res['word_count'] for res in results]\n",
    "times = [res['time'] for res in results]\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(sizes, times)\n",
    "plt.title('Зависимость времени анализа от размера текста')\n",
    "plt.xlabel('Количество слов в тексте')\n",
    "plt.ylabel('Время анализа (сек)')\n",
    "plt.savefig('time_vs_size.png')\n",
    "plt.show()\n",
    "\n",
    "pos_distribution = get_pos_distribution(largest_dict)\n",
    "print(\"Распределение частей речи:\")\n",
    "for pos, counts in pos_distribution.items():\n",
    "    print(f\"{pos}: {counts['unique']} уникальных слов, {counts['total']} всего слов\")\n",
    "\n",
    "pos_names = list(pos_distribution.keys())\n",
    "unique_counts = [pos_distribution[pos]['unique'] for pos in pos_names]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(pos_names, unique_counts)\n",
    "plt.title('Распределение частей речи')\n",
    "plt.xlabel('Часть речи')\n",
    "plt.ylabel('Количество уникальных слов')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig('pos_distribution.png')\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[81]\u001B[39m\u001B[32m, line 25\u001B[39m\n\u001B[32m     22\u001B[39m     text = f.read()\n\u001B[32m     24\u001B[39m start_time = time.time()\n\u001B[32m---> \u001B[39m\u001B[32m25\u001B[39m dictionary = \u001B[43mget_dictionary\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     26\u001B[39m elapsed_time = time.time() - start_time\n\u001B[32m     28\u001B[39m results.append({\n\u001B[32m     29\u001B[39m     \u001B[33m'\u001B[39m\u001B[33mfile\u001B[39m\u001B[33m'\u001B[39m: file,\n\u001B[32m     30\u001B[39m     \u001B[33m'\u001B[39m\u001B[33msize_bytes\u001B[39m\u001B[33m'\u001B[39m: os.path.getsize(file),\n\u001B[32m   (...)\u001B[39m\u001B[32m     33\u001B[39m     \u001B[33m'\u001B[39m\u001B[33mtime\u001B[39m\u001B[33m'\u001B[39m: elapsed_time\n\u001B[32m     34\u001B[39m })\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[76]\u001B[39m\u001B[32m, line 4\u001B[39m, in \u001B[36mget_dictionary\u001B[39m\u001B[34m(text)\u001B[39m\n\u001B[32m      3\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mget_dictionary\u001B[39m(text):\n\u001B[32m----> \u001B[39m\u001B[32m4\u001B[39m     parsed = \u001B[43mparse_text\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      5\u001B[39m     word_freq = Counter()\n\u001B[32m      6\u001B[39m     lemmas_info = {}\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[74]\u001B[39m\u001B[32m, line 5\u001B[39m, in \u001B[36mparse_text\u001B[39m\u001B[34m(text)\u001B[39m\n\u001B[32m      3\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mparse_text\u001B[39m(text):\n\u001B[32m      4\u001B[39m     m = Mystem(disambiguation=\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m----> \u001B[39m\u001B[32m5\u001B[39m     analysis = \u001B[43mm\u001B[49m\u001B[43m.\u001B[49m\u001B[43manalyze\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      6\u001B[39m     result = []\n\u001B[32m      8\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m item \u001B[38;5;129;01min\u001B[39;00m analysis:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\m_nlp_course_vyatsu\\Lib\\site-packages\\pymystem3\\mystem.py:250\u001B[39m, in \u001B[36mMystem.analyze\u001B[39m\u001B[34m(self, text)\u001B[39m\n\u001B[32m    248\u001B[39m result = []\n\u001B[32m    249\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m line \u001B[38;5;129;01min\u001B[39;00m text.splitlines():\n\u001B[32m--> \u001B[39m\u001B[32m250\u001B[39m     result.extend(\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_analyze_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43mline\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[32m    251\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\m_nlp_course_vyatsu\\Lib\\site-packages\\pymystem3\\mystem.py:313\u001B[39m, in \u001B[36mMystem._analyze_impl\u001B[39m\u001B[34m(self, text)\u001B[39m\n\u001B[32m    310\u001B[39m \u001B[38;5;28mself\u001B[39m._procin.write(text)\n\u001B[32m    311\u001B[39m \u001B[38;5;28mself\u001B[39m._procin.write(_NL)\n\u001B[32m--> \u001B[39m\u001B[32m313\u001B[39m out, _ = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_proc\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcommunicate\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    314\u001B[39m \u001B[38;5;28mself\u001B[39m._proc = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    315\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    316\u001B[39m     \u001B[38;5;66;03m#obj = json.loads(out)\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\m_nlp_course_vyatsu\\Lib\\subprocess.py:1222\u001B[39m, in \u001B[36mPopen.communicate\u001B[39m\u001B[34m(self, input, timeout)\u001B[39m\n\u001B[32m   1219\u001B[39m     endtime = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1221\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1222\u001B[39m     stdout, stderr = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_communicate\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mendtime\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1223\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m:\n\u001B[32m   1224\u001B[39m     \u001B[38;5;66;03m# https://bugs.python.org/issue25942\u001B[39;00m\n\u001B[32m   1225\u001B[39m     \u001B[38;5;66;03m# See the detailed comment in .wait().\u001B[39;00m\n\u001B[32m   1226\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m timeout \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\m_nlp_course_vyatsu\\Lib\\subprocess.py:1644\u001B[39m, in \u001B[36mPopen._communicate\u001B[39m\u001B[34m(self, input, endtime, orig_timeout)\u001B[39m\n\u001B[32m   1640\u001B[39m \u001B[38;5;66;03m# Wait for the reader threads, or time out.  If we time out, the\u001B[39;00m\n\u001B[32m   1641\u001B[39m \u001B[38;5;66;03m# threads remain reading and the fds left open in case the user\u001B[39;00m\n\u001B[32m   1642\u001B[39m \u001B[38;5;66;03m# calls communicate again.\u001B[39;00m\n\u001B[32m   1643\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.stdout \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1644\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mstdout_thread\u001B[49m\u001B[43m.\u001B[49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_remaining_time\u001B[49m\u001B[43m(\u001B[49m\u001B[43mendtime\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1645\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.stdout_thread.is_alive():\n\u001B[32m   1646\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m TimeoutExpired(\u001B[38;5;28mself\u001B[39m.args, orig_timeout)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\m_nlp_course_vyatsu\\Lib\\threading.py:1094\u001B[39m, in \u001B[36mThread.join\u001B[39m\u001B[34m(self, timeout)\u001B[39m\n\u001B[32m   1091\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m timeout \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m   1092\u001B[39m     timeout = \u001B[38;5;28mmax\u001B[39m(timeout, \u001B[32m0\u001B[39m)\n\u001B[32m-> \u001B[39m\u001B[32m1094\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_handle\u001B[49m\u001B[43m.\u001B[49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 81
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ta_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
