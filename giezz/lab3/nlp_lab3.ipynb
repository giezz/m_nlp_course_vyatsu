{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JSgPwQvpCx3D"
   },
   "source": [
    "# Лабораторная работа 3. TF-IDF взвешивание терминов для векторизации текстов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BJMAWHiLCx3G"
   },
   "source": [
    "**Задание 1.** Реализуйте векторизацию текстов на основе tf-idf метода взвешивания терминов. Предусмотрите задание размера словаря терминов, используемого для взвешивания. Можно код организовать отдельным модулем. Протестируйте свою реализацию метода."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oXxlxgntCx3H"
   },
   "source": [
    "**Задание 2.** Сделайте классификацию новостных текстов из предыдущей лабораторной работы. Векторизация tf-idf имеет два этапа: построение словаря и построение векторов, поэтому исходный датасет надо разделить на две части. Первая часть будет использоваться для формирования словаря, а вторая часть для обучения и проверки модели классификации.\n",
    "\n",
    "Основные действия:\n",
    "- предобработка (токенизация, удаление пунктуации, лематизация),\n",
    "- разбиение на выборку текстов на две части в отношении 1:1 (первую используйте для построения словаря, а вторую часть - для построения модели классификации),\n",
    "- построение словаря для tf-idf векторизации по первой части выборки (**на основе вашей реализации tf-idf**),\n",
    "- векторизация текстов второй части выборки (**на основе вашей реализации tf-idf**),\n",
    "- классификация текстов по темам на основе **логистической регрессии** (вторую часть выборки надо снова разделит две части:обучающую и валидационную),\n",
    "- оценивание качество классификации."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from giezz.lab3 import tfidf\n",
    "from natasha import (\n",
    "    Segmenter,\n",
    "    MorphVocab,\n",
    "    NewsEmbedding,\n",
    "    NewsMorphTagger,\n",
    "    NewsSyntaxParser,\n",
    "    NewsNERTagger,\n",
    "    Doc\n",
    ")\n",
    "import re\n",
    "\n",
    "segmenter = Segmenter()\n",
    "morph_vocab = MorphVocab()\n",
    "emb = NewsEmbedding()\n",
    "morph_tagger = NewsMorphTagger(emb)\n",
    "syntax_parser = NewsSyntaxParser(emb)\n",
    "ner_tagger = NewsNERTagger(emb)\n",
    "\n",
    "def preprocess_text_natasha(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    text = re.sub(r'\\d+', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip().lower()\n",
    "    doc = Doc(text)\n",
    "    doc.segment(segmenter)\n",
    "    doc.tag_morph(morph_tagger)\n",
    "    tokens = []\n",
    "    for token in doc.tokens:\n",
    "        token.lemmatize(morph_vocab)\n",
    "        lemma = token.lemma.lower().strip()\n",
    "        if len(lemma) > 2 and not lemma.isdigit():\n",
    "            tokens.append(lemma)\n",
    "\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "df = pd.read_csv('../../labs/lenta_ru_news_filtered.csv')\n",
    "df['processed_text'] = df['text'].apply(preprocess_text_natasha)\n",
    "df = df[df['processed_text'].str.len() > 0]\n",
    "df_for_dict, df_for_classifier = train_test_split(df, test_size=0.5, random_state=42, stratify=df['topic'])\n",
    "\n",
    "print(f\"Размер первой части (для словаря): {len(df_for_dict)}\")\n",
    "print(f\"Размер второй части (для классификации): {len(df_for_classifier)}\")\n",
    "\n",
    "vectorizer = tfidf.TfidfVectorizer(max_features=500)\n",
    "vectorizer.fit(df_for_dict['processed_text'].tolist())\n",
    "\n",
    "print(f\"Размер словаря: {len(vectorizer.vocabulary)}\")\n",
    "\n",
    "X_second = vectorizer.transform(df_for_classifier['processed_text'].tolist())\n",
    "y_second = df_for_classifier['topic'].values\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_second, y_second, test_size=0.3, random_state=42, stratify=y_second)\n",
    "\n",
    "print(f\"Обучающая выборка: {X_train.shape[0]} примеров\")\n",
    "print(f\"Валидационная выборка: {X_val.shape[0]} примеров\")\n",
    "\n",
    "model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, y_pred))\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "topic_counts = pd.Series(y_val).value_counts()\n",
    "topic_accuracy = {}\n",
    "\n",
    "for topic in topic_counts.index:\n",
    "    mask = y_val == topic\n",
    "    if mask.sum() > 0:\n",
    "        topic_accuracy[topic] = accuracy_score(y_val[mask], y_pred[mask])\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "topic_counts.plot(kind='bar', title='Распределение тем в валидационной выборке')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "pd.Series(topic_accuracy).plot(kind='bar', title='Accuracy по темам')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W25jEDvqCx3I"
   },
   "source": "**Задание 3.** Используя модуль TfIdfVectorizer библиотеки sklearn, сделайте классификацию новостных текстов из предыдущей лабораторной работы. Предусмотрите предобработку текстов и задание ограничение словаря при взвешивании. Оцените качество классификации."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "sklearn_vectorizer = TfidfVectorizer(max_features=500)\n",
    "X_second_sklearn = sklearn_vectorizer.fit_transform(df_for_classifier['processed_text'])\n",
    "\n",
    "\n",
    "X_train_sk, X_val_sk, y_train_sk, y_val_sk = train_test_split(\n",
    "    X_second_sklearn, y_second, test_size=0.3, random_state=42, stratify=y_second\n",
    ")\n",
    "\n",
    "model_sk = LogisticRegression(random_state=42, max_iter=1000)\n",
    "model_sk.fit(X_train_sk, y_train_sk)\n",
    "\n",
    "y_pred_sk = model_sk.predict(X_val_sk)\n",
    "\n",
    "accuracy_sk = accuracy_score(y_val_sk, y_pred_sk)\n",
    "print(f\"Accuracy (sklearn): {accuracy_sk:.4f}\")\n",
    "print(\"\\nClassification Report (sklearn):\")\n",
    "print(classification_report(y_val_sk, y_pred_sk))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Задание 4.** Постройте график зависимости значения `accuracy` от размера словаря, по которому выполняется tf-idf векторизация. Для этого используйте первую и вторую выборки исходного датасета, которые были получены при первом разбиении в предыдущем задании. По первой части текстов один раз строите словарь `D_common`, потом из него формируете словарь `D_k` размером `k`. По словарю `D_k` векторизуете вторую часть текстов, затем по которой обучаете и проверяете качество модели. Значение `k` меняется от [500, 1000, 1500, 2000, 3000, 5000]."
   ],
   "metadata": {
    "id": "sXpA1BOEEGAM"
   }
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "k_values = [500, 1000, 1500, 2000, 3000, 5000]\n",
    "accuracies = []\n",
    "\n",
    "for k in k_values:\n",
    "    print(f\"\\nРазмер словаря: {k}\")\n",
    "    vectorizer_k = tfidf.TfidfVectorizer(max_features=k)\n",
    "    vectorizer_k.fit(df_for_dict['processed_text'].tolist())\n",
    "    X_k = vectorizer_k.transform(df_for_classifier['processed_text'].tolist())\n",
    "    X_train_k, X_val_k, y_train_k, y_val_k = train_test_split(\n",
    "        X_k, y_second, test_size=0.3, random_state=42, stratify=y_second\n",
    "    )\n",
    "\n",
    "    model_k = LogisticRegression(random_state=42, max_iter=1000)\n",
    "    model_k.fit(X_train_k, y_train_k)\n",
    "\n",
    "    y_pred_k = model_k.predict(X_val_k)\n",
    "    accuracy_k = accuracy_score(y_val_k, y_pred_k)\n",
    "    accuracies.append(accuracy_k)\n",
    "\n",
    "    print(f\"Accuracy для k={k}: {accuracy_k:.4f}\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_values, accuracies, 'bo-', linewidth=2, markersize=8)\n",
    "plt.xlabel('Размер словаря (k)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Зависимость accuracy от размера словаря TF-IDF')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "for i, (k, acc) in enumerate(zip(k_values, accuracies)):\n",
    "    plt.annotate(f'{acc:.3f}', (k, acc), textcoords=\"offset points\",\n",
    "                 xytext=(0,10), ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    'Размер словаря': k_values,\n",
    "    'Accuracy': accuracies\n",
    "})\n",
    "print(\"\\nРезультаты:\")\n",
    "print(results_df.to_string(index=False))\n",
    "best_idx = np.argmax(accuracies)\n",
    "print(f\"\\nЛучший результат: k={k_values[best_idx]}, accuracy={accuracies[best_idx]:.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "elgLZbrMCx3J"
   },
   "source": [
    "**Задание 5.** Реализуйте поиск новостных текстов по текстовому запросу, задаваемому пользователем, на основе tf-idf векторизации (можно использовать любую реализацию методов)."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "full_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "full_tfidf_matrix = full_vectorizer.fit_transform(df['processed_text'])\n",
    "\n",
    "def search_news(query, vectorizer, tfidf_matrix, original_df, top_k=5):\n",
    "    processed_query = preprocess_text_natasha(query)\n",
    "    query_vector = vectorizer.transform([processed_query])\n",
    "    similarities = cosine_similarity(query_vector, tfidf_matrix).flatten()\n",
    "    top_indices = similarities.argsort()[-top_k:][::-1]\n",
    "    results = []\n",
    "    for i, idx in enumerate(top_indices):\n",
    "        if similarities[idx] > 0:\n",
    "            results.append({\n",
    "                'rank': i + 1,\n",
    "                'title': original_df.iloc[idx]['title'],\n",
    "                'topic': original_df.iloc[idx]['topic'],\n",
    "                'similarity': round(similarities[idx], 4),\n",
    "                'text_preview': original_df.iloc[idx]['text'][:200] + '...' if len(original_df.iloc[idx]['text']) > 200 else original_df.iloc[idx]['text']\n",
    "            })\n",
    "\n",
    "    return results"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "user_query = \"поставки спирта\".strip()\n",
    "print(f\"\\nРезультаты поиска по запросу: '{user_query}'\")\n",
    "search_results = search_news(user_query, full_vectorizer, full_tfidf_matrix, df, top_k=5)\n",
    "\n",
    "if search_results:\n",
    "    for result in search_results:\n",
    "        print(f\"\\n{result['rank']}. [{result['topic']}] {result['title']}\")\n",
    "        print(f\"   Схожесть: {result['similarity']}\")\n",
    "        print(f\"   Фрагмент: {result['text_preview']}\")\n",
    "else:\n",
    "    print(\"по вашему запросу ничего не найдено.\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ta_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
