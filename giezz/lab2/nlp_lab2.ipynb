{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RObpMk4tNe2s"
   },
   "source": [
    "# Лабораторная работа 2. Классификация текстов на основе вхождения в документ словарных слов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dPtFkBosNe22"
   },
   "source": [
    "**Задание 1.** Загрузите в датафрейм новостной датасет `lenta_ru_news_filtered.csv`, собранный на базе корпуса `lenta.ru v1.0`. В датасете каждая новость описывается следующими полями:\n",
    "* **url** - адрес новости на сайте `lenta.ru`,\n",
    "* **topic** - тема новости,\n",
    "* **title** - заголовок новости,\n",
    "* **text** - текст новости.\n",
    "\n",
    "Ответьте на следуюшие вопросы:\n",
    "1. Сколько всего новостных текстов?\n",
    "2. На какие темы встречаются новости?\n",
    "3. Сколько новостных текстов в каждой теме?"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../../labs/lenta_ru_news_filtered.csv')\n",
    "\n",
    "total_news = len(df)\n",
    "print(f\"1. Всего новостных текстов: {total_news}\")\n",
    "\n",
    "topics = df['topic'].unique()\n",
    "print(f\"\\n2. Темы новостей:\\n{topics}\")\n",
    "\n",
    "news_per_topic = df['topic'].value_counts()\n",
    "print(f\"\\n3. Количество новостей по темам:\\n{news_per_topic}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jBTuCxFKNe22"
   },
   "source": [
    "**Задание 2.** Выполните предобработку новостных текстов в виде:\n",
    "- приведение к нижнему регистру,\n",
    "- удаление знаков пунктуации.\n",
    "\n",
    "**(!) Далее в лабораторной работе задания выполняются с полученными обработанными текстами**.\n",
    "\n",
    "Разделите датасет на обучающую и тестовую части в соотношении 80% к 20%. Выведите диаграммы, отражающие количество текстов по каждой теме в каждой из частей."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    return text\n",
    "\n",
    "df['processed_text'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "print(\"Пример исходного текста:\")\n",
    "print(df['text'].iloc[0][:200])\n",
    "print(\"\\nПример обработанного текста:\")\n",
    "print(df['processed_text'].iloc[0][:200])\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['topic'])\n",
    "\n",
    "print(f\"\\nРазмер обучающей выборки: {len(train_df)}\")\n",
    "print(f\"Размер тестовой выборки: {len(test_df)}\")\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "train_topic_counts = train_df['topic'].value_counts()\n",
    "ax1.bar(train_topic_counts.index, train_topic_counts.values)\n",
    "ax1.set_title('Распределение тем в обучающей выборке')\n",
    "ax1.set_xlabel('Темы')\n",
    "ax1.set_ylabel('Количество новостей')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "test_topic_counts = test_df['topic'].value_counts()\n",
    "ax2.bar(test_topic_counts.index, test_topic_counts.values)\n",
    "ax2.set_title('Распределение тем в тестовой выборке')\n",
    "ax2.set_xlabel('Темы')\n",
    "ax2.set_ylabel('Количество новостей')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Детальная статистика по распределению\n",
    "print(\"\\nРаспределение по темам в обучающей выборке:\")\n",
    "print(train_df['topic'].value_counts())\n",
    "print(\"\\nРаспределение по темам в тестовой выборке:\")\n",
    "print(test_df['topic'].value_counts())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "train_df",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PK0zMLkjNe22"
   },
   "source": [
    "**Задание 3.** Подсчитайте частоту встречаемости слов предобработанных новостных текстов `обучающей` части датафрейма. Какие слова употребляются наиболее часто вцелом в этих новостных текстах, а какие слова употребляются в этих же новостных текстах относительно тем (выведите топ-`50` слов для каждого случая)?\n",
    "\n",
    "Нахождение частот слов в новостных текстах датафрейма можно выполнять посредством инструмента `FreqDist` библиотеки `NLTK`.\n",
    "\n",
    "Например:\n",
    "```\n",
    "df['text'].apply(lambda x: nltk.FreqDist(nltk.word_tokenize(x)))\n",
    "```"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "from nltk import FreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Скачиваем необходимые ресурсы NLTK (если еще не скачаны)\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Используем обучающую выборку из задания 2\n",
    "print(\"Подсчет частоты слов в обучающей выборке...\")\n",
    "\n",
    "# 1. Частота слов во всей обучающей выборке\n",
    "all_words = []\n",
    "for text in train_df['processed_text']:\n",
    "    tokens = word_tokenize(text)\n",
    "    all_words.extend(tokens)\n",
    "\n",
    "# Создаем объект FreqDist для всех слов\n",
    "freq_dist_all = FreqDist(all_words)\n",
    "\n",
    "# Выводим топ-50 самых частых слов\n",
    "print(\"Топ-50 самых частых слов во всей обучающей выборке:\")\n",
    "top_50_all = freq_dist_all.most_common(50)\n",
    "for i, (word, freq) in enumerate(top_50_all, 1):\n",
    "    print(f\"{i:2d}. {word}: {freq}\")\n",
    "\n",
    "# Визуализация топ-50 слов всей выборки\n",
    "plt.figure(figsize=(12, 8))\n",
    "words, frequencies = zip(*top_50_all)\n",
    "plt.bar(range(len(words)), frequencies)\n",
    "plt.xticks(range(len(words)), words, rotation=45, ha='right')\n",
    "plt.title('Топ-50 самых частых слов в обучающей выборке')\n",
    "plt.xlabel('Слова')\n",
    "plt.ylabel('Частота')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. Частота слов по темам\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Частота слов по темам:\")\n",
    "\n",
    "# Получаем уникальные темы\n",
    "topics = train_df['topic'].unique()\n",
    "\n",
    "# Создаем словарь для хранения FreqDist по каждой теме\n",
    "freq_dist_by_topic = {}\n",
    "\n",
    "for topic in topics:\n",
    "    print(f\"\\nТема: {topic}\")\n",
    "\n",
    "    # Фильтруем тексты по теме\n",
    "    topic_texts = train_df[train_df['topic'] == topic]['processed_text']\n",
    "\n",
    "    # Собираем все слова для данной темы\n",
    "    topic_words = []\n",
    "    for text in topic_texts:\n",
    "        tokens = word_tokenize(text)\n",
    "        topic_words.extend(tokens)\n",
    "\n",
    "    # Создаем FreqDist для текущей темы\n",
    "    freq_dist_topic = FreqDist(topic_words)\n",
    "    freq_dist_by_topic[topic] = freq_dist_topic\n",
    "\n",
    "    # Выводим топ-50 слов для темы\n",
    "    top_50_topic = freq_dist_topic.most_common(50)\n",
    "    print(f\"Топ-50 слов в теме '{topic}':\")\n",
    "    for i, (word, freq) in enumerate(top_50_topic, 1):\n",
    "        print(f\"{i:2d}. {word}: {freq}\")\n",
    "\n",
    "    # Визуализация для каждой темы\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    words_topic, freqs_topic = zip(*top_50_topic)\n",
    "    plt.bar(range(len(words_topic)), freqs_topic)\n",
    "    plt.xticks(range(len(words_topic)), words_topic, rotation=45, ha='right')\n",
    "    plt.title(f'Топ-50 слов в теме: {topic}')\n",
    "    plt.xlabel('Слова')\n",
    "    plt.ylabel('Частота')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Дополнительная информация\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Дополнительная статистика:\")\n",
    "print(f\"Всего уникальных слов во всей обучающей выборке: {len(freq_dist_all)}\")\n",
    "\n",
    "for topic in topics:\n",
    "    freq_dist = freq_dist_by_topic[topic]\n",
    "    print(f\"Уникальных слов в теме '{topic}': {len(freq_dist)}\")\n",
    "    print(f\"Общее количество слов в теме '{topic}': {freq_dist.N()}\")\n",
    "\n",
    "# Сохраняем результаты для использования в следующих заданиях\n",
    "topic_word_freq = {}\n",
    "for topic in topics:\n",
    "    topic_word_freq[topic] = dict(freq_dist_by_topic[topic].most_common())\n",
    "\n",
    "# Выводим несколько примеров самых характерных слов для каждой темы\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Самые характерные слова по темам (первые 10 из топ-50):\")\n",
    "for topic in topics:\n",
    "    top_words = list(topic_word_freq[topic].keys())[:10]\n",
    "    print(f\"{topic}: {', '.join(top_words)}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kB6aTP1CNe23"
   },
   "source": [
    "**Задание 4.** Составьте словари из текстов `обучающей` части датасетов наболее часто встречающихся слов, которые встречаются **только в одной из каждых тем**. Постройте диаграмму для топ-`50` этих слов для каждой темы (ось X - слова, ось Y - частоты встречаемости слов в новостных текстах)."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# Создаем словари уникальных слов для каждой темы\n",
    "print(\"Создание словарей слов, встречающихся только в одной теме...\")\n",
    "\n",
    "# Собираем все слова из всех тем\n",
    "all_topic_words = defaultdict(set)\n",
    "for topic in topics:\n",
    "    # Берем топ-500 слов из каждой темы для анализа\n",
    "    top_words = [word for word, freq in freq_dist_by_topic[topic].most_common(500)]\n",
    "    all_topic_words[topic] = set(top_words)\n",
    "\n",
    "# Находим слова, уникальные для каждой темы\n",
    "unique_words_per_topic = {}\n",
    "\n",
    "for topic in topics:\n",
    "    # Слова текущей темы\n",
    "    current_topic_words = all_topic_words[topic]\n",
    "\n",
    "    # Слова всех других тем\n",
    "    other_topics_words = set()\n",
    "    for other_topic in topics:\n",
    "        if other_topic != topic:\n",
    "            other_topics_words.update(all_topic_words[other_topic])\n",
    "\n",
    "    # Находим слова, которые есть только в текущей теме\n",
    "    unique_words = current_topic_words - other_topics_words\n",
    "\n",
    "    # Создаем словарь {слово: частота} для уникальных слов\n",
    "    unique_words_freq = {}\n",
    "    for word in unique_words:\n",
    "        unique_words_freq[word] = freq_dist_by_topic[topic][word]\n",
    "\n",
    "    # Сортируем по частоте и берем топ-50\n",
    "    sorted_unique_words = sorted(unique_words_freq.items(), key=lambda x: x[1], reverse=True)[:50]\n",
    "    unique_words_per_topic[topic] = sorted_unique_words\n",
    "\n",
    "    print(f\"\\nТема: {topic}\")\n",
    "    print(f\"Количество уникальных слов: {len(unique_words)}\")\n",
    "    print(f\"Топ-10 уникальных слов: {[word for word, freq in sorted_unique_words[:10]]}\")\n",
    "\n",
    "# Визуализация топ-50 уникальных слов для каждой темы\n",
    "for topic in topics:\n",
    "    if unique_words_per_topic[topic]:\n",
    "        plt.figure(figsize=(15, 8))\n",
    "\n",
    "        words = [item[0] for item in unique_words_per_topic[topic]]\n",
    "        frequencies = [item[1] for item in unique_words_per_topic[topic]]\n",
    "\n",
    "        # Создаем столбчатую диаграмму\n",
    "        bars = plt.bar(range(len(words)), frequencies, color='skyblue')\n",
    "\n",
    "        # Настройки графика\n",
    "        plt.title(f'Топ-50 уникальных слов для темы: {topic}', fontsize=16)\n",
    "        plt.xlabel('Слова', fontsize=12)\n",
    "        plt.ylabel('Частота встречаемости', fontsize=12)\n",
    "        plt.xticks(range(len(words)), words, rotation=45, ha='right', fontsize=10)\n",
    "\n",
    "        # Добавляем значения на столбцы\n",
    "        for bar, freq in zip(bars, frequencies):\n",
    "            height = bar.get_height()\n",
    "            plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{freq}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Выводим таблицу с данными\n",
    "        print(f\"\\nТоп-50 уникальных слов для темы '{topic}':\")\n",
    "        df_unique = pd.DataFrame(unique_words_per_topic[topic], columns=['Слово', 'Частота'])\n",
    "        print(df_unique.to_string(index=False))\n",
    "    else:\n",
    "        print(f\"\\nДля темы '{topic}' не найдено уникальных слов среди топ-500 слов\")\n",
    "\n",
    "# Анализ результатов\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"АНАЛИЗ РЕЗУЛЬТАТОВ:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for topic in topics:\n",
    "    unique_count = len(unique_words_per_topic[topic])\n",
    "    total_unique_words = sum([len(unique_words_per_topic[t]) for t in topics])\n",
    "    print(f\"Тема '{topic}': {unique_count} уникальных слов ({unique_count/total_unique_words*100:.1f}% от общего числа уникальных слов)\")\n",
    "\n",
    "# Сохраняем словари для использования в следующем задании\n",
    "print(\"\\nСохранение словарей уникальных слов...\")\n",
    "\n",
    "# Создаем отдельные словари для каждой темы\n",
    "topic_vocabularies = {}\n",
    "for topic in topics:\n",
    "    topic_vocabularies[topic] = [word for word, freq in unique_words_per_topic[topic]]\n",
    "\n",
    "# Выводим примеры самых характерных уникальных слов для каждой темы\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"САМЫЕ ХАРАКТЕРНЫЕ УНИКАЛЬНЫЕ СЛОВА ПО ТЕМАМ:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for topic in topics:\n",
    "    if unique_words_per_topic[topic]:\n",
    "        top_10_unique = [word for word, freq in unique_words_per_topic[topic][:10]]\n",
    "        print(f\"{topic}: {', '.join(top_10_unique)}\")\n",
    "    else:\n",
    "        print(f\"{topic}: не найдено уникальных слов\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qptkq3vdNe23"
   },
   "source": [
    "**Задание 5.** Выполните классификацию новостных текстов из `тестовой` части датасета на основе `top-k слов` (`k` - как входной параметр) из словарей, построенных в задании `4` на основе `обучающей` части датасета. Оцените показатели `полнота` и `точность` классификации (относительно значения параметра `k`) при указании количества документов, для которых не получилось определить тему.\n",
    "\n",
    "Общий алгоритм классификации:\n",
    "\n",
    "- Для документа `x` считаем сколько встречается слов, из словаря каждой из тем.\n",
    "- Выбираем тему для документа `x` с максимальным количеством слов\n",
    "- Отдельно обрабатываем случаи, когда количество слов одинаковое или равно 0. Это случаи, когда не получилось определить тему документа."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7G45Gf3INe23"
   },
   "source": [
    "**Задание 6.** Познакомьтесь с библиотекой Natasha для обработки текстов на русском языке, прочитав <a href=\"https://habr.com/ru/articles/516098/\">статью</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ORckot5FNe23"
   },
   "source": [
    "#### Импорт библиотек и создание объекта для работы с текстом\n",
    "Импортируйте библиотеки для сегментации на предложения, морфологического и синтаксического анализа.\n",
    "\n",
    "```python\n",
    "from natasha import(\n",
    "    Segmenter,\n",
    "    MorphVocab,\n",
    "    NewsEmbedding,\n",
    "    NewsMorphTagger,\n",
    "    NewsSyntaxParser,\n",
    "    Doc,\n",
    ")\n",
    "```\n",
    "\n",
    "Создайте объект из обрабатываемого текста `text`:\n",
    "\n",
    "```python\n",
    "text_doc = Doc(text)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wv2SnFnKNe23"
   },
   "source": [
    "#### Сегментация на предложения\n",
    "Создайте объект, который будет выполнять сегментацию текста на предложения:\n",
    "\n",
    "```python\n",
    "segmenter = Segmenter()\n",
    "```\n",
    "\n",
    "С помощью сегментатора можно разбить текст на предложения и токены:\n",
    "\n",
    "```python\n",
    "text_doc.segment(segmenter)\n",
    "print(text_doc.tokens)\n",
    "print(text_doc.sents)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o0QJCWd5Ne23"
   },
   "source": [
    "#### Морфологический анализ\n",
    "\n",
    "Инициализируйте словарь и морфологический анализатор:\n",
    "```python\n",
    "morph_vocab = MorphVocab()\n",
    "morph_tagger = NewsMorphTagger(emb)\n",
    "```\n",
    "\n",
    "Чтобы получить морфологическую разметку слова `word` воспользуйтесь:\n",
    "\n",
    "```python\n",
    "morph_vocab.parse(word)\n",
    "```\n",
    "\n",
    "Добавить морфологическую разметку к объекту обрабатываемого текста (Doc) можно как\n",
    "\n",
    "```python\n",
    "text_doc.tag_morph(morph_tagger)\n",
    "print(text_doc.tokens)\n",
    "text_doc.sents[0].morph.print()\n",
    "```\n",
    "\n",
    "Получить леммы текста можно через метод `lemmatize`:\n",
    "\n",
    "```python\n",
    "for token in text_doc.tokens:\n",
    "    token.lemmatize(morph_vocab)\n",
    "{_.text: _.lemma for _ in text_doc.tokens}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PE1lOP42Ne23"
   },
   "source": [
    "**Задание 7.** Добавьте столбец в обучающую и тестовую часть датасета с обработанными текстами после лемматизации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v_lrR-9zNe24"
   },
   "source": [
    "**Задание 8.** Составьте словари на основе текстов `обучающей` части датасета по аналогии с **заданием 4.**, но с текстами, полученными после лемматзации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UMaAd55sNe24"
   },
   "source": [
    "**Задание 9.** Сделайте классификацию новостных текстов `тестовой` части датасета по аналогии с **заданием 5.**, но с текстами, полученными **после лемматзации**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ta_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
