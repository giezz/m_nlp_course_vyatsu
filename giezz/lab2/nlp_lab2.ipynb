{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RObpMk4tNe2s"
   },
   "source": [
    "# Лабораторная работа 2. Классификация текстов на основе вхождения в документ словарных слов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dPtFkBosNe22"
   },
   "source": [
    "**Задание 1.** Загрузите в датафрейм новостной датасет `lenta_ru_news_filtered.csv`, собранный на базе корпуса `lenta.ru v1.0`. В датасете каждая новость описывается следующими полями:\n",
    "* **url** - адрес новости на сайте `lenta.ru`,\n",
    "* **topic** - тема новости,\n",
    "* **title** - заголовок новости,\n",
    "* **text** - текст новости.\n",
    "\n",
    "Ответьте на следуюшие вопросы:\n",
    "1. Сколько всего новостных текстов?\n",
    "2. На какие темы встречаются новости?\n",
    "3. Сколько новостных текстов в каждой теме?"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../../labs/lenta_ru_news_filtered.csv')\n",
    "\n",
    "total_news = len(df)\n",
    "print(f\"1. Всего новостных текстов: {total_news}\")\n",
    "\n",
    "topics = df['topic'].unique()\n",
    "print(f\"\\n2. Темы новостей:\\n{topics}\")\n",
    "\n",
    "news_per_topic = df['topic'].value_counts()\n",
    "print(f\"\\n3. Количество новостей по темам:\\n{news_per_topic}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jBTuCxFKNe22"
   },
   "source": [
    "**Задание 2.** Выполните предобработку новостных текстов в виде:\n",
    "- приведение к нижнему регистру,\n",
    "- удаление знаков пунктуации.\n",
    "\n",
    "**(!) Далее в лабораторной работе задания выполняются с полученными обработанными текстами**.\n",
    "\n",
    "Разделите датасет на обучающую и тестовую части в соотношении 80% к 20%. Выведите диаграммы, отражающие количество текстов по каждой теме в каждой из частей."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    return text\n",
    "\n",
    "df['processed_text'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "print(\"Пример исходного текста:\")\n",
    "print(df['text'].iloc[0][:200])\n",
    "print(\"\\nПример обработанного текста:\")\n",
    "print(df['processed_text'].iloc[0][:200])\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['topic'])\n",
    "\n",
    "print(f\"\\nРазмер обучающей выборки: {len(train_df)}\")\n",
    "print(f\"Размер тестовой выборки: {len(test_df)}\")\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "train_topic_counts = train_df['topic'].value_counts()\n",
    "ax1.bar(train_topic_counts.index, train_topic_counts.values)\n",
    "ax1.set_title('Распределение тем в обучающей выборке')\n",
    "ax1.set_xlabel('Темы')\n",
    "ax1.set_ylabel('Количество новостей')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "test_topic_counts = test_df['topic'].value_counts()\n",
    "ax2.bar(test_topic_counts.index, test_topic_counts.values)\n",
    "ax2.set_title('Распределение тем в тестовой выборке')\n",
    "ax2.set_xlabel('Темы')\n",
    "ax2.set_ylabel('Количество новостей')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Детальная статистика по распределению\n",
    "print(\"\\nРаспределение по темам в обучающей выборке:\")\n",
    "print(train_df['topic'].value_counts())\n",
    "print(\"\\nРаспределение по темам в тестовой выборке:\")\n",
    "print(test_df['topic'].value_counts())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "train_df",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PK0zMLkjNe22"
   },
   "source": [
    "**Задание 3.** Подсчитайте частоту встречаемости слов предобработанных новостных текстов `обучающей` части датафрейма. Какие слова употребляются наиболее часто вцелом в этих новостных текстах, а какие слова употребляются в этих же новостных текстах относительно тем (выведите топ-`50` слов для каждого случая)?\n",
    "\n",
    "Нахождение частот слов в новостных текстах датафрейма можно выполнять посредством инструмента `FreqDist` библиотеки `NLTK`.\n",
    "\n",
    "Например:\n",
    "```\n",
    "df['text'].apply(lambda x: nltk.FreqDist(nltk.word_tokenize(x)))\n",
    "```"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "from nltk import FreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Скачиваем необходимые ресурсы NLTK (если еще не скачаны)\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Используем обучающую выборку из задания 2\n",
    "print(\"Подсчет частоты слов в обучающей выборке...\")\n",
    "\n",
    "# 1. Частота слов во всей обучающей выборке\n",
    "all_words = []\n",
    "for text in train_df['processed_text']:\n",
    "    tokens = word_tokenize(text)\n",
    "    all_words.extend(tokens)\n",
    "\n",
    "# Создаем объект FreqDist для всех слов\n",
    "freq_dist_all = FreqDist(all_words)\n",
    "\n",
    "# Выводим топ-50 самых частых слов\n",
    "print(\"Топ-50 самых частых слов во всей обучающей выборке:\")\n",
    "top_50_all = freq_dist_all.most_common(50)\n",
    "for i, (word, freq) in enumerate(top_50_all, 1):\n",
    "    print(f\"{i:2d}. {word}: {freq}\")\n",
    "\n",
    "# Визуализация топ-50 слов всей выборки\n",
    "plt.figure(figsize=(12, 8))\n",
    "words, frequencies = zip(*top_50_all)\n",
    "plt.bar(range(len(words)), frequencies)\n",
    "plt.xticks(range(len(words)), words, rotation=45, ha='right')\n",
    "plt.title('Топ-50 самых частых слов в обучающей выборке')\n",
    "plt.xlabel('Слова')\n",
    "plt.ylabel('Частота')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. Частота слов по темам\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Частота слов по темам:\")\n",
    "\n",
    "# Получаем уникальные темы\n",
    "topics = train_df['topic'].unique()\n",
    "\n",
    "# Создаем словарь для хранения FreqDist по каждой теме\n",
    "freq_dist_by_topic = {}\n",
    "\n",
    "for topic in topics:\n",
    "    print(f\"\\nТема: {topic}\")\n",
    "\n",
    "    # Фильтруем тексты по теме\n",
    "    topic_texts = train_df[train_df['topic'] == topic]['processed_text']\n",
    "\n",
    "    # Собираем все слова для данной темы\n",
    "    topic_words = []\n",
    "    for text in topic_texts:\n",
    "        tokens = word_tokenize(text)\n",
    "        topic_words.extend(tokens)\n",
    "\n",
    "    # Создаем FreqDist для текущей темы\n",
    "    freq_dist_topic = FreqDist(topic_words)\n",
    "    freq_dist_by_topic[topic] = freq_dist_topic\n",
    "\n",
    "    # Выводим топ-50 слов для темы\n",
    "    top_50_topic = freq_dist_topic.most_common(50)\n",
    "    print(f\"Топ-50 слов в теме '{topic}':\")\n",
    "    for i, (word, freq) in enumerate(top_50_topic, 1):\n",
    "        print(f\"{i:2d}. {word}: {freq}\")\n",
    "\n",
    "    # Визуализация для каждой темы\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    words_topic, freqs_topic = zip(*top_50_topic)\n",
    "    plt.bar(range(len(words_topic)), freqs_topic)\n",
    "    plt.xticks(range(len(words_topic)), words_topic, rotation=45, ha='right')\n",
    "    plt.title(f'Топ-50 слов в теме: {topic}')\n",
    "    plt.xlabel('Слова')\n",
    "    plt.ylabel('Частота')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Дополнительная информация\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Дополнительная статистика:\")\n",
    "print(f\"Всего уникальных слов во всей обучающей выборке: {len(freq_dist_all)}\")\n",
    "\n",
    "for topic in topics:\n",
    "    freq_dist = freq_dist_by_topic[topic]\n",
    "    print(f\"Уникальных слов в теме '{topic}': {len(freq_dist)}\")\n",
    "    print(f\"Общее количество слов в теме '{topic}': {freq_dist.N()}\")\n",
    "\n",
    "# Сохраняем результаты для использования в следующих заданиях\n",
    "topic_word_freq = {}\n",
    "for topic in topics:\n",
    "    topic_word_freq[topic] = dict(freq_dist_by_topic[topic].most_common())\n",
    "\n",
    "# Выводим несколько примеров самых характерных слов для каждой темы\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Самые характерные слова по темам (первые 10 из топ-50):\")\n",
    "for topic in topics:\n",
    "    top_words = list(topic_word_freq[topic].keys())[:10]\n",
    "    print(f\"{topic}: {', '.join(top_words)}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kB6aTP1CNe23"
   },
   "source": [
    "**Задание 4.** Составьте словари из текстов `обучающей` части датасетов наболее часто встречающихся слов, которые встречаются **только в одной из каждых тем**. Постройте диаграмму для топ-`50` этих слов для каждой темы (ось X - слова, ось Y - частоты встречаемости слов в новостных текстах)."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# Создаем словари уникальных слов для каждой темы\n",
    "print(\"Создание словарей слов, встречающихся только в одной теме...\")\n",
    "\n",
    "# Собираем все слова из всех тем\n",
    "all_topic_words = defaultdict(set)\n",
    "for topic in topics:\n",
    "    # Берем топ-500 слов из каждой темы для анализа\n",
    "    top_words = [word for word, freq in freq_dist_by_topic[topic].most_common(500)]\n",
    "    all_topic_words[topic] = set(top_words)\n",
    "\n",
    "# Находим слова, уникальные для каждой темы\n",
    "unique_words_per_topic = {}\n",
    "\n",
    "for topic in topics:\n",
    "    # Слова текущей темы\n",
    "    current_topic_words = all_topic_words[topic]\n",
    "\n",
    "    # Слова всех других тем\n",
    "    other_topics_words = set()\n",
    "    for other_topic in topics:\n",
    "        if other_topic != topic:\n",
    "            other_topics_words.update(all_topic_words[other_topic])\n",
    "\n",
    "    # Находим слова, которые есть только в текущей теме\n",
    "    unique_words = current_topic_words - other_topics_words\n",
    "\n",
    "    # Создаем словарь {слово: частота} для уникальных слов\n",
    "    unique_words_freq = {}\n",
    "    for word in unique_words:\n",
    "        unique_words_freq[word] = freq_dist_by_topic[topic][word]\n",
    "\n",
    "    # Сортируем по частоте и берем топ-50\n",
    "    sorted_unique_words = sorted(unique_words_freq.items(), key=lambda x: x[1], reverse=True)[:50]\n",
    "    unique_words_per_topic[topic] = sorted_unique_words\n",
    "\n",
    "    print(f\"\\nТема: {topic}\")\n",
    "    print(f\"Количество уникальных слов: {len(unique_words)}\")\n",
    "    print(f\"Топ-10 уникальных слов: {[word for word, freq in sorted_unique_words[:10]]}\")\n",
    "\n",
    "# Визуализация топ-50 уникальных слов для каждой темы\n",
    "for topic in topics:\n",
    "    if unique_words_per_topic[topic]:\n",
    "        plt.figure(figsize=(15, 8))\n",
    "\n",
    "        words = [item[0] for item in unique_words_per_topic[topic]]\n",
    "        frequencies = [item[1] for item in unique_words_per_topic[topic]]\n",
    "\n",
    "        # Создаем столбчатую диаграмму\n",
    "        bars = plt.bar(range(len(words)), frequencies, color='skyblue')\n",
    "\n",
    "        # Настройки графика\n",
    "        plt.title(f'Топ-50 уникальных слов для темы: {topic}', fontsize=16)\n",
    "        plt.xlabel('Слова', fontsize=12)\n",
    "        plt.ylabel('Частота встречаемости', fontsize=12)\n",
    "        plt.xticks(range(len(words)), words, rotation=45, ha='right', fontsize=10)\n",
    "\n",
    "        # Добавляем значения на столбцы\n",
    "        for bar, freq in zip(bars, frequencies):\n",
    "            height = bar.get_height()\n",
    "            plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{freq}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Выводим таблицу с данными\n",
    "        print(f\"\\nТоп-50 уникальных слов для темы '{topic}':\")\n",
    "        df_unique = pd.DataFrame(unique_words_per_topic[topic], columns=['Слово', 'Частота'])\n",
    "        print(df_unique.to_string(index=False))\n",
    "    else:\n",
    "        print(f\"\\nДля темы '{topic}' не найдено уникальных слов среди топ-500 слов\")\n",
    "\n",
    "# Анализ результатов\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"АНАЛИЗ РЕЗУЛЬТАТОВ:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for topic in topics:\n",
    "    unique_count = len(unique_words_per_topic[topic])\n",
    "    total_unique_words = sum([len(unique_words_per_topic[t]) for t in topics])\n",
    "    print(f\"Тема '{topic}': {unique_count} уникальных слов ({unique_count/total_unique_words*100:.1f}% от общего числа уникальных слов)\")\n",
    "\n",
    "# Сохраняем словари для использования в следующем задании\n",
    "print(\"\\nСохранение словарей уникальных слов...\")\n",
    "\n",
    "# Создаем отдельные словари для каждой темы\n",
    "topic_vocabularies = {}\n",
    "for topic in topics:\n",
    "    topic_vocabularies[topic] = [word for word, freq in unique_words_per_topic[topic]]\n",
    "\n",
    "# Выводим примеры самых характерных уникальных слов для каждой темы\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"САМЫЕ ХАРАКТЕРНЫЕ УНИКАЛЬНЫЕ СЛОВА ПО ТЕМАМ:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for topic in topics:\n",
    "    if unique_words_per_topic[topic]:\n",
    "        top_10_unique = [word for word, freq in unique_words_per_topic[topic][:10]]\n",
    "        print(f\"{topic}: {', '.join(top_10_unique)}\")\n",
    "    else:\n",
    "        print(f\"{topic}: не найдено уникальных слов\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qptkq3vdNe23"
   },
   "source": [
    "**Задание 5.** Выполните классификацию новостных текстов из `тестовой` части датасета на основе `top-k слов` (`k` - как входной параметр) из словарей, построенных в задании `4` на основе `обучающей` части датасета. Оцените показатели `полнота` и `точность` классификации (относительно значения параметра `k`) при указании количества документов, для которых не получилось определить тему.\n",
    "\n",
    "Общий алгоритм классификации:\n",
    "\n",
    "- Для документа `x` считаем сколько встречается слов, из словаря каждой из тем.\n",
    "- Выбираем тему для документа `x` с максимальным количеством слов\n",
    "- Отдельно обрабатываем случаи, когда количество слов одинаковое или равно 0. Это случаи, когда не получилось определить тему документа."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_score, recall_score, classification_report\n",
    "from collections import Counter\n",
    "\n",
    "def classify_documents(test_df, topic_vocabularies, k):\n",
    "    \"\"\"\n",
    "    Классификация документов на основе top-k уникальных слов для каждой темы\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for idx, row in test_df.iterrows():\n",
    "        text = row['processed_text']\n",
    "        true_topic = row['topic']\n",
    "\n",
    "        # Считаем вхождения слов из каждого словаря\n",
    "        topic_scores = {}\n",
    "\n",
    "        for topic, vocab in topic_vocabularies.items():\n",
    "            # Берем top-k слов из словаря темы\n",
    "            top_k_words = vocab[:k]\n",
    "\n",
    "            # Подсчитываем, сколько из этих слов встречается в тексте\n",
    "            word_count = 0\n",
    "            for word in top_k_words:\n",
    "                if word in text:\n",
    "                    word_count += 1\n",
    "\n",
    "            topic_scores[topic] = word_count\n",
    "\n",
    "        # Находим тему с максимальным количеством совпадений\n",
    "        max_score = max(topic_scores.values())\n",
    "\n",
    "        if max_score == 0:\n",
    "            # Не удалось определить тему\n",
    "            predicted_topic = 'unknown'\n",
    "        else:\n",
    "            # Находим все темы с максимальным счетом\n",
    "            best_topics = [topic for topic, score in topic_scores.items() if score == max_score]\n",
    "\n",
    "            if len(best_topics) == 1:\n",
    "                predicted_topic = best_topics[0]\n",
    "            else:\n",
    "                # Несколько тем с одинаковым максимальным счетом\n",
    "                predicted_topic = 'unknown'\n",
    "\n",
    "        results.append({\n",
    "            'true_topic': true_topic,\n",
    "            'predicted_topic': predicted_topic,\n",
    "            'text_id': idx\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def evaluate_classification(results_df, k_values):\n",
    "    \"\"\"\n",
    "    Оценка качества классификации для разных значений k\n",
    "    \"\"\"\n",
    "    evaluation_results = []\n",
    "\n",
    "    for k in k_values:\n",
    "        print(f\"\\nОценка классификации для k = {k}\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        # Фильтруем результаты для текущего k\n",
    "        current_results = results_df[results_df['k'] == k].copy()\n",
    "\n",
    "        # Разделяем на определенные и неопределенные темы\n",
    "        known_results = current_results[current_results['predicted_topic'] != 'unknown']\n",
    "        unknown_results = current_results[current_results['predicted_topic'] == 'unknown']\n",
    "\n",
    "        # Статистика\n",
    "        total_docs = len(current_results)\n",
    "        known_docs = len(known_results)\n",
    "        unknown_docs = len(unknown_results)\n",
    "        unknown_ratio = unknown_docs / total_docs\n",
    "\n",
    "        print(f\"Всего документов: {total_docs}\")\n",
    "        print(f\"Документов с определенной темой: {known_docs}\")\n",
    "        print(f\"Документов с неопределенной темой: {unknown_docs}\")\n",
    "        print(f\"Доля неопределенных: {unknown_ratio:.3f}\")\n",
    "\n",
    "        if known_docs > 0:\n",
    "            # Вычисляем точность и полноту только для определенных документов\n",
    "            y_true = known_results['true_topic']\n",
    "            y_pred = known_results['predicted_topic']\n",
    "\n",
    "            # Общая точность\n",
    "            accuracy = (y_true == y_pred).mean()\n",
    "\n",
    "            # Precision и recall по классам\n",
    "            precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "            recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "\n",
    "            print(f\"Точность (accuracy) на определенных документах: {accuracy:.3f}\")\n",
    "            print(f\"Precision (взвешенное): {precision:.3f}\")\n",
    "            print(f\"Recall (взвешенное): {recall:.3f}\")\n",
    "\n",
    "            # Детальный отчет по классам\n",
    "            print(\"\\nДетальный отчет по классам:\")\n",
    "            print(classification_report(y_true, y_pred, zero_division=0))\n",
    "        else:\n",
    "            accuracy = precision = recall = 0\n",
    "            print(\"Нет документов с определенной темой для оценки метрик\")\n",
    "\n",
    "        evaluation_results.append({\n",
    "            'k': k,\n",
    "            'total_docs': total_docs,\n",
    "            'known_docs': known_docs,\n",
    "            'unknown_docs': unknown_docs,\n",
    "            'unknown_ratio': unknown_ratio,\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(evaluation_results)\n",
    "\n",
    "k_values = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50]\n",
    "all_results = []\n",
    "\n",
    "for k in k_values:\n",
    "    print(f\"\\nКлассификация для k = {k}\")\n",
    "    classification_results = classify_documents(test_df, topic_vocabularies, k)\n",
    "    classification_results['k'] = k\n",
    "\n",
    "    all_results.append(classification_results)\n",
    "\n",
    "all_results_df = pd.concat(all_results, ignore_index=True)\n",
    "evaluation_df = evaluate_classification(all_results_df, k_values)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(evaluation_df['k'], evaluation_df['accuracy'], marker='o', label='Accuracy', linewidth=2)\n",
    "plt.plot(evaluation_df['k'], evaluation_df['precision'], marker='s', label='Precision', linewidth=2)\n",
    "plt.plot(evaluation_df['k'], evaluation_df['recall'], marker='^', label='Recall', linewidth=2)\n",
    "plt.xlabel('k (количество слов из словаря)')\n",
    "plt.ylabel('Метрика')\n",
    "plt.title('Метрики качества классификации')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(evaluation_df['k'], evaluation_df['unknown_ratio'], marker='o', color='red', linewidth=2)\n",
    "plt.xlabel('k (количество слов из словаря)')\n",
    "plt.ylabel('Доля неопределенных')\n",
    "plt.title('Доля документов с неопределенной темой')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(evaluation_df['k'], evaluation_df['total_docs'], marker='o', label='Всего', linewidth=2)\n",
    "plt.plot(evaluation_df['k'], evaluation_df['known_docs'], marker='s', label='Определенные', linewidth=2)\n",
    "plt.plot(evaluation_df['k'], evaluation_df['unknown_docs'], marker='^', label='Неопределенные', linewidth=2)\n",
    "plt.xlabel('k (количество слов из словаря)')\n",
    "plt.ylabel('Количество документов')\n",
    "plt.title('Распределение документов по категориям')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "f1_scores = 2 * (evaluation_df['precision'] * evaluation_df['recall']) / (evaluation_df['precision'] + evaluation_df['recall'] + 1e-10)\n",
    "plt.plot(evaluation_df['k'], f1_scores, marker='o', color='purple', linewidth=2)\n",
    "plt.xlabel('k (количество слов из словаря)')\n",
    "plt.ylabel('F1-мера')\n",
    "plt.title('F1-мера (взвешенная)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "best_idx = evaluation_df['accuracy'].idxmax()\n",
    "best_k = evaluation_df.loc[best_idx, 'k']\n",
    "best_accuracy = evaluation_df.loc[best_idx, 'accuracy']\n",
    "\n",
    "print(f\"\\nbest_k: {best_k}\")\n",
    "print(f\"best_accuracy: {best_accuracy:.3f}\")\n",
    "\n",
    "print(f\"\\nДЕТАЛЬНЫЙ АНАЛИЗ ДЛЯ k = {best_k}:\")\n",
    "\n",
    "best_k_results = all_results_df[all_results_df['k'] == best_k]\n",
    "confusion_data = []\n",
    "\n",
    "for true_topic in best_k_results['true_topic'].unique():\n",
    "    for pred_topic in best_k_results['predicted_topic'].unique():\n",
    "        count = len(best_k_results[\n",
    "            (best_k_results['true_topic'] == true_topic) &\n",
    "            (best_k_results['predicted_topic'] == pred_topic)\n",
    "        ])\n",
    "        if count > 0:\n",
    "            confusion_data.append({\n",
    "                'true_topic': true_topic,\n",
    "                'predicted_topic': pred_topic,\n",
    "                'count': count\n",
    "            })\n",
    "\n",
    "confusion_df = pd.DataFrame(confusion_data)\n",
    "pivot_df = confusion_df.pivot(index='true_topic', columns='predicted_topic', values='count').fillna(0)\n",
    "\n",
    "print(\"Матрица confusion:\")\n",
    "print(pivot_df)\n",
    "\n",
    "print(f\"\\nЭФФЕКТИВНОСТЬ ПО ТЕМАМ ДЛЯ k = {best_k}:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "topic_analysis = []\n",
    "for topic in best_k_results['true_topic'].unique():\n",
    "    topic_data = best_k_results[best_k_results['true_topic'] == topic]\n",
    "    total = len(topic_data)\n",
    "    correct = len(topic_data[topic_data['predicted_topic'] == topic])\n",
    "    unknown = len(topic_data[topic_data['predicted_topic'] == 'unknown'])\n",
    "    wrong = total - correct - unknown\n",
    "\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "\n",
    "    topic_analysis.append({\n",
    "        'topic': topic,\n",
    "        'total': total,\n",
    "        'correct': correct,\n",
    "        'wrong': wrong,\n",
    "        'unknown': unknown,\n",
    "        'accuracy': accuracy\n",
    "    })\n",
    "\n",
    "topic_analysis_df = pd.DataFrame(topic_analysis)\n",
    "print(topic_analysis_df.to_string(index=False))\n",
    "\n",
    "# Сохранение результатов\n",
    "print(f\"\\nСохранение результатов...\")\n",
    "all_results_df.to_csv('classification_results.csv', index=False)\n",
    "evaluation_df.to_csv('evaluation_metrics.csv', index=False)\n",
    "\n",
    "print(\"ЗАДАНИЕ 5 ВЫПОЛНЕНО!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7G45Gf3INe23"
   },
   "source": [
    "**Задание 6.** Познакомьтесь с библиотекой Natasha для обработки текстов на русском языке, прочитав <a href=\"https://habr.com/ru/articles/516098/\">статью</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ORckot5FNe23"
   },
   "source": [
    "#### Импорт библиотек и создание объекта для работы с текстом\n",
    "Импортируйте библиотеки для сегментации на предложения, морфологического и синтаксического анализа.\n",
    "\n",
    "```python\n",
    "from natasha import(\n",
    "    Segmenter,\n",
    "    MorphVocab,\n",
    "    NewsEmbedding,\n",
    "    NewsMorphTagger,\n",
    "    NewsSyntaxParser,\n",
    "    Doc,\n",
    ")\n",
    "```\n",
    "\n",
    "Создайте объект из обрабатываемого текста `text`:\n",
    "\n",
    "```python\n",
    "text_doc = Doc(text)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wv2SnFnKNe23"
   },
   "source": [
    "#### Сегментация на предложения\n",
    "Создайте объект, который будет выполнять сегментацию текста на предложения:\n",
    "\n",
    "```python\n",
    "segmenter = Segmenter()\n",
    "```\n",
    "\n",
    "С помощью сегментатора можно разбить текст на предложения и токены:\n",
    "\n",
    "```python\n",
    "text_doc.segment(segmenter)\n",
    "print(text_doc.tokens)\n",
    "print(text_doc.sents)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o0QJCWd5Ne23"
   },
   "source": [
    "#### Морфологический анализ\n",
    "\n",
    "Инициализируйте словарь и морфологический анализатор:\n",
    "```python\n",
    "morph_vocab = MorphVocab()\n",
    "morph_tagger = NewsMorphTagger(emb)\n",
    "```\n",
    "\n",
    "Чтобы получить морфологическую разметку слова `word` воспользуйтесь:\n",
    "\n",
    "```python\n",
    "morph_vocab.parse(word)\n",
    "```\n",
    "\n",
    "Добавить морфологическую разметку к объекту обрабатываемого текста (Doc) можно как\n",
    "\n",
    "```python\n",
    "text_doc.tag_morph(morph_tagger)\n",
    "print(text_doc.tokens)\n",
    "text_doc.sents[0].morph.print()\n",
    "```\n",
    "\n",
    "Получить леммы текста можно через метод `lemmatize`:\n",
    "\n",
    "```python\n",
    "for token in text_doc.tokens:\n",
    "    token.lemmatize(morph_vocab)\n",
    "{_.text: _.lemma for _ in text_doc.tokens}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PE1lOP42Ne23"
   },
   "source": [
    "**Задание 7.** Добавьте столбец в обучающую и тестовую часть датасета с обработанными текстами после лемматизации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v_lrR-9zNe24"
   },
   "source": [
    "**Задание 8.** Составьте словари на основе текстов `обучающей` части датасета по аналогии с **заданием 4.**, но с текстами, полученными после лемматзации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UMaAd55sNe24"
   },
   "source": [
    "**Задание 9.** Сделайте классификацию новостных текстов `тестовой` части датасета по аналогии с **заданием 5.**, но с текстами, полученными **после лемматзации**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ta_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
